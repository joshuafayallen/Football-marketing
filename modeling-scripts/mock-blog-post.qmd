---
title: "What Marketing Taught Me About Football"
subtitle: "And What Football Taught Me about Marketing"
format: html
jupyter: python3
---

## Introduction 

The rise of McVay's use of 13 personnel made me think of an old [*Athletic Football Show*](https://youtu.be/93RzplSc3mM?si=_2p7P7xEF0g43bxu&t=3395) episode where they pointed out the use of auxiliary runs to get to explosives. This got me thinking about how can we model the returns on NFL plays.  A major talking point in the 2025 season was Sean McVay's usage of 13 personnel to force another linebacker onto the field. The benefit of this is that they offense has heavier bodies on the field to help the run game but you are keeping more eligible receivers on the field. Since Kyle Shanahan and Sean McVay started as head coaches 8 years ago they have been two of the best play callers and play designers in the game. One way that their offenses are so good is that they find different ways to generate explosive plays. Typically explosive plays are defined as runs of ten plus yards or pass plays of twenty plus yards. Effectively you are gaining the equivalent of one plus first down on one play shortening the field and reducing the number of plays you have to run, or a quick touchdown. 

There are a variety of ways that Kyle Shanahan and Sean McVay try to manipulate defenses. The most obvious ways that they do this is the use of pre-snap motion. Pre-snap motion can help the QB check into the right play by tipping whether a defense is in man or zone. Motion directly before the snap forces the defense to adjust run and pass assignments on the fly creating and getting offensive players up to top speed before they start the play. 


Another way that Kyle and Sean  try to manufacture explosive plays is by using personnel packages in creative ways. Each personnel package has certain advantage but, generally, the idea is that you want a certain defenseive personnel grouping on the field and leverage their weaknesses. To make sure we are on the same page on what different personnel packages mean I have broken down the most common ones in @tbl-personnel. Conventionally personnel pacakges are broken down by the number of RBs on the field and the number of TEs on the field.

```{python}
#| echo: false
#| label: tbl-personnel 
#| tbl-cap: "Common Personnel Packages"
import polars as pl 
import polars.selectors as cs
from nflreadpy import load_pbp
from nflreadpy import load_snap_counts
from great_tables import GT

te_df = pl.DataFrame(
    {'Number of TEs': [1,2,3]}
)

rb_df = pl.DataFrame(
    {'Number of RBs': [1,2,3]}
)
personnel_df = rb_df.join(te_df, how = 'cross')
personnel_df = (rb_df
    .join(te_df, how = 'cross')
    .with_columns(
        pl.concat_str([
            'Number of RBs',
            'Number of TEs'
        ]).alias('Personnel Package'))
    .with_columns(
        pl.col('Personnel Package').str.to_integer().alias('p')
    )   
        .filter(pl.col('p') <= 22)
        .drop('p')
)

GT(personnel_df)

```


Since the days of the big neckroll the league has shifted. Due to changes in the rules passing has massively upticked. As a result defenses have tried to combat this schematically by playing with less defenders in the box, more defensive backs, and more complex pass coverages. Early on Kyle and Sean used a lot of under-center play action so defenses would simply change the picture on the quarterback when their backs were turned. To counter this Kyle has used a lot of 21 personnel to force a conflict between defending the run with lighter bodies or defending the pass with heavier bodies. 

With all the talk of 13 personnel this year and ton of talk about pre-snap motion from the past few years I got kind of curious. What are the returns of some of these schematic trends and how can we model it? This is an interesting question for a variety of reasons. Should we focus on the design and mechanics of the play or should we focus more on what the bodies on the field get you? Well that kind of depends on a lot of things and as was the case for me what data you can access. Since I only have participation data I decided to look at explosive plays as a function of personnel grouping. 

## How Can We Model This? 

There are lots of ways we can model this but because play callers have these different personel packages that they can use my mind immediately went to Media Mixed Modeling(MMMs). What is an MMM? Lets say you are in a marketing department at a large electronics company with a 1 million dollar budget to spend on advertising. You spend that million dollars across various marketing channels whether this search optimization, TV, email marketing, or influencer marketing.^[There are many more places and ways to advertise] You then see that sales increased that month, but you want to know which channel was responsible for this spike so you can invest your money more wisely. In the early digital marketing age companies relied on multi-touch attribution models. The general idea of this was that if a consumer saw an ad on Facebook clicked on it and got the product. You could track those clicks back to consumer and then estimate a model to better undertand where to put your money. 

However, around 2020 as a response to privacy concerns, laws passed by various governing bodies, and IOS14 these models started to break. More people started using ad blockers, Apple made tracking these attribution points much harder, and some people started using browsers that restricted access to what companies could track. In short, following the customer behavior got a lot harder. So marketing analytics turned to good old fashion experiments and MMMs. The general idea of an MMM is that weekly conversions, sales, revenue, etc are a function of

$$
Sales_t = \alpha + \overbrace{\sum^{m}_{m=1}\beta_{m}F(x^{*}_{t,m})}^{\text{Impact of marketing}} + \overbrace{\sum^{c}_{c=1} \gamma_{c} Z_{t,c}}^{\text{Impact of Controls}} + \varepsilon_{t}
$$


We can then go ahead and measure the relative contribution of each channel and forecast forward on where we should spend our money and about how much money we should spend on that channel. As you may imagine there are lots of gnarly time dynamics in marketing that you have to deal with. As an electronics company executive you would anticipate that every year in late October to early November sales effectively just stop as people wait for Black Friday sales. You also anticipate that there is only so many ads you can run on the same customer base before they stop caring. 

Snapping back to football this made me think of personnel groupings as channels. You more or less have a fixed number of plays per game which would track with a marketing budget. The play design may be more akin to a specific ads on each of these platforms to adapt to the user base and platform specific rules.^[Admittedly this is a pretty big simplifying assumption.] In addition, both marketing and football deal with really noisy data that try to capture complex behavioral dynamics and you do not have a ton of data to work with generally. Based on this idea I decided to try and build an MMM to understand how play-callers should invest their play budget. Perhaps just as importantly this felt like a fun opportunity to figure out how to build an MMM. 


# Building A Regular MMM

## Making The Data 

For those only interested in the Football stuff you should just skip to the next section. For those sticking around for I am just working through the example provided by [PyMC-Marketing](https://www.pymc-marketing.io/en/stable/notebooks/mmm/mmm_example.html).^[Where this is going to differ is I am going to use Polars instead of Pandas.] We are going to generate some synthetic data for a made up company at the weekly level over two years. This roughly equates to `{python} 52*2` weeks of data.

```{python}
#| echo: true
#| code-fold: true
#| label: mod-import

import arviz as az 
import matplotlib.pyplot as plt
import numpy as np 
import pandas as pd
import preliz as pz
import pymc as pm
import seaborn as sns
from pymc_extras.prior import Prior
from pymc_marketing.mmm import GeometricAdstock, LogisticSaturation
from pymc_marketing.mmm.multidimensional import MMM
from pymc_marketing.mmm.transformers import geometric_adstock, logistic_saturation
from datetime import date
# seed from random.org

seed = 39233615
rng: np.random.Generator = np.random.default_rng(seed = seed)

min_date = date(2018, 4,1)
max_date= date(2021, 9, 1)

```

### Media Costs 

First we are going to generate two years worth of data 


```{python}

df = (pl.DataFrame({
    'date_week': pl.date_range(min_date, max_date, "1d", eager = True)}
    )
    .filter(pl.col("date_week").dt.weekday() == 1)
    .with_columns(
        pl.col('date_week').dt.year().alias('year'),
        pl.col('date_week').dt.month().alias('month'),
        pl.col('date_week').dt.ordinal_day().alias('day_of_year')
    )
)


```

Next we are going to generate synthetic data for two channels. Which are going to be a bit different based on carryover and the saturation parameters. Roughly the costs for the each of the channels look like this. 


```{python}
#| code-fold: true
n = df.height
x1 = rng.uniform(low = 0.0, high = 1.0, size = n)
x2 = rng.uniform(low = 0.0, high = 1.0, size = n)


df = (
    df
    .with_columns(
        x1_raw = pl.Series(x1), 
        x2_raw = pl.Series(x2)
    )
    .with_columns(
        pl.when(pl.col('x1_raw') > 0.9)
        .then(pl.col('x1_raw'))
        .otherwise((pl.col('x1_raw')/2))
        .alias('x1'),
        pl.when(pl.col('x2_raw') >0.8)
        .then(pl.col("x2_raw"))
        .otherwise(0)
        .alias('x2')
    )
    .drop(['x1_raw', 'x2_raw'])
)

df.columns

long_data = df.unpivot(on = ['x1','x2'], index = 'date_week')

fig, ax = plt.subplots()
sns.lineplot(data = long_data, x = 'date_week', y = 'value', hue = 'variable', alpha = 0.5)

```

Next we are going to pass each of these through our adstock function. In media marketing our adstock is effectively our carryover effect. If you see an ad one day how long is that going to stay with you? 


```{python}
#| code-fold: True
alpha1: float = 0.4 
alpha2: float = 0.2

df = (
    df
    .with_columns(
        pl.col('x1')
        .map_batches(
            lambda s: geometric_adstock(
                x = s.to_numpy(),
                alpha = alpha1,
                l_max = 8,
                normalize=True
            ).eval().flatten(), 
            return_dtype=pl.Float64
        ).alias('x1_adstock'),
        pl.col('x2')
        .map_batches(
            lambda s: geometric_adstock(
                x = s.to_numpy(),
                alpha = alpha2,
                l_max = 8,
                normalize=True
            ).eval().flatten(), 
            return_dtype=pl.Float64
        ).alias('x2_adstock')
    )
)


```

The next thing we are going to do is add our concentration effects or how much money we spending in each chanel. 


```{python}
#| code-fold: true
lam1: float = 4.0
lam2: float = 3.0

df = (
    df
    .with_columns(
        pl.col('x1_adstock')
        .map_batches(
            lambda s: 
            logistic_saturation(
                x = s.to_numpy(),
                lam = lam1
            ).eval(), 
            return_dtype=pl.Float64
        ).alias('x1_saturated_adstock'), 
        pl.col('x2_adstock')
        .map_batches(
            lambda s: 
            logistic_saturation(
                x = s.to_numpy(),
                lam = lam1
            ).eval(), 
            return_dtype=pl.Float64
        ).alias('x2_saturated_adstock')
    )
)


```

Then as always we are going to plot this to make sure everything is good. 


```{python}
long_effects = df.unpivot(on = cs.starts_with('x'), index = 'date_week')

fig, ax = plt.subplots()
g = sns.FacetGrid(data= long_effects, col = 'variable', col_wrap = 2)
g.map(sns.lineplot, 'date_week', 'value')

```

This is looks like the costs are pretty realistic. 


### Trend and Seasonal Parts 

Now we are going to add some seasonality 



```{python}
#| code-fold: true
import math 
df = df.with_columns(
    trend=(
        pl.linear_space(0.0, 50.0, n)  # sequence 0..50 with n samples
        .add(10.0)
        .pow(1.0 / 4.0)
        .sub(1.0)
    ),
    cs=(
        -(2.0 * 2.0 * math.pi * pl.col("day_of_year") / 365.5).sin()
    ),
    cc=(
        (1.0 * 2.0 * math.pi * pl.col("day_of_year") / 365.5).cos()
    ),
).with_columns(
    seasonality=0.5 * (pl.col("cs") + pl.col("cc"))
)

fig, ax = plt.subplots()
sns.lineplot(x="date_week", y="trend", color="C2", label="trend", data=df, ax=ax)
sns.lineplot(
    x="date_week", y="seasonality", color="C3", label="seasonality", data=df, ax=ax
)
ax.legend(loc="upper left")
ax.set(xlabel="date", ylabel=None)
ax.set_title("Trend & Seasonality Components", fontsize=18, fontweight="bold");
```


### Making Controls 

Finally, we will just add our dependent variable and our controls 



```{python}
#| code-fold: true
epsilon = rng.normal(loc = 0.0, scale = 0.25, size = n)
amplitude = 1 
beta_1 = 3.0
beta_2 = 2.0
betas = [beta_1, beta_2]




df = (
    df
    .with_columns(
        pl.lit(2.0).alias('intercept'),
        pl.Series(epsilon).alias('epsilon'), 
        pl.lit(beta_1).alias('beta_1'), 
        pl.lit(beta_2).alias('beta_2'), 
        event_1=(pl.col("date_week") == date(2019, 5,13)).cast(pl.Float64),
        event_2=(pl.col("date_week") == date(2020, 9, 14)).cast(pl.Float64),
    )
    .with_columns(

    )
    .with_columns(
        (
            pl.col('intercept')
            + pl.col('trend')
            + pl.col("seasonality")
            + pl.lit(1.5) * pl.col('event_1')
            + pl.lit(2.5) * pl.col('event_2')
            + pl.col('beta_1') * pl.col('x1_saturated_adstock')
            + pl.col('beta_2') * pl.col('x2_saturated_adstock')
            + pl.col('epsilon')
        ).alias('y')
    )
)

fig, ax = plt.subplots()
sns.lineplot(x="date_week", y="y", color="black", data=df, ax=ax)
ax.set(xlabel="date", ylabel="y (thousands)")


```

[We can now go ahead and look at the ground truth](https://www.pymc-marketing.io/en/stable/notebooks/mmm/mmm_example.html), but that is an excercise left for the reader. 


## Building The Regular MMM

For whatever reason I found that working with PyMC is just easier to do in pandas so we are going to throw it over to Pandas. 


```{python}

df_pd = df.to_pandas()
columns_to_keep = [
    "date_week",
    "y",
    "x1",
    "x2",
    "event_1",
    "event_2",
    "day_of_year",
]

mod_df = df_pd[columns_to_keep]

mod_df['t'] = range(n)

```


The whole stick of using Bayesian stats is to set appropriate priors. We know at least a few things 

    - Channel Contributions are positive and vary. So we are going to set a 1 sigma per channel. How much they vary is an open question. The wider the prior the more area we give the model to explore. This has some pros and cons.
    - We kind of expect channels where we spend the most to have more attributed sales. We wouldn't advertise a product targeted for an older audience on TikTok as an example. 


The `MMM` class in `PyMC-Marketing` passes parts of your data through the `MaxABs` scaler in `scikit-learn`. So we have to express our priors in on this scale. So what would normally be a weakly informative prior may actually be super informative. 


```{python}

from sklearn.preprocessing import MaxAbsScaler
scaler = MaxAbsScaler()

examp = (
    df
    .with_columns(
        pl.col('x1_saturated_adstock')
        .map_batches(
            lambda s: scaler.fit_transform(s.to_numpy().reshape(-1,1)).ravel(),
            return_dtype = pl.Float64
        ).alias('scaled_version')
    )
)

long = examp.unpivot(on = ['x1_saturated_adstock', 'scaled_version'], index= 'date_week')

fig,ax = plt.subplots()
sns.scatterplot(data = long, x = 'date_week', y= 'value', hue = 'variable')


```


We are going to define our sigma priors as a share of what we spend on them. 


```{python}

spend_per_channel = mod_df[['x1', 'x2']].sum(axis = 0)

spend_share =  spend_per_channel/spend_per_channel.sum()

sigma_prior = 2 * spend_share.to_numpy()

X = mod_df.drop('y', axis = 1)

y = mod_df['y']
```


Next we are just going to define some priors. 


```{python}
my_priors= {
    "intercept": Prior("Normal", mu=0.5, sigma=0.2),
    "adstock_alpha": Prior("Beta", alpha=1, beta=3, dims="channel"),
    "saturation_beta": Prior("HalfNormal", sigma=sigma_prior, dims="channel"),
    "saturation_lam": Prior("Gamma", alpha=3, beta=1, dims="channel"),
    "gamma_control": Prior("Normal", mu=0, sigma=0.05, dims="control"),
    "gamma_fourier": Prior("Laplace", mu=0, b=0.2, dims="fourier_mode"),
    "likelihood": Prior("Normal", sigma=Prior("HalfNormal", sigma=6)),
}

samp_config = {'progressbar': True, 'random_seed': seed, 'nuts_sampler': 'numpyro'}

mmm = MMM(
    model_config=my_priors,
    sampler_config=samp_config,
    date_column="date_week",
    adstock=GeometricAdstock(l_max=8),
    saturation=LogisticSaturation(),
    channel_columns=["x1", "x2"],
    control_columns=["event_1", "event_2", "t"],
    yearly_seasonality=2,
    target_column = 'y'
)
mmm.build_model(X,y)

mmm.add_original_scale_contribution_variable(
    var=[
        "channel_contribution",
        "control_contribution",
        "intercept_contribution",
        "yearly_seasonality_contribution",
        "y",
    ]
)


```


As always we should look a the prior predictive to make sure things look good 


```{python}

mmm.sample_prior_predictive(X,y)

fig,axes = mmm.plot.prior_predictive()

```


Since we have the ability to build a model where we know the true DGP we know these priors are pretty weakly informative. Basically all this means is that we want to give the model a large enough space to explore, but not so big that we let the model go from $-\infty$ to $\infty$

## What are some of these parameters?

So the entire crux of MMMs is that we don't expect ads: to work instantly, every additional ad to increase sales, and the effect of an ad to persist that much. Thee multiple avenues to see growth in the outcome of interest was what initially got me thinking about using these models. But when I started digging more into these models these time dynamics became what got me really excited to see how this could work. In this section I am going to borrow heavily from some excellent blog posts by [Ryan O'Sullivan](https://towardsdatascience.com/mastering-marketing-mix-modelling-in-python-7bbfe31360f9/?source=post_page-----49dce1a5b33d---------------------------------------)


### Adstock

Adstock really just refers to the decay rate. So if I run an advertisement how quickly do those effects diminsh overtime. Adstock is typically conceptualized as a ratio where  . Typically adstock is constrained to be between zero and 1 so the Beta distribution is used more often than not. A vagueish prior would be a $adstock \sim \beta(\alpha = 1, \beta = 3)$ or something that looks like this 

```{python}
fig, axe = plt.subplots()
pz.Beta(alpha = 1, beta = 3).plot_pdf()

```


To build an intuition on what that may mean practically we can simulate some spend


```{python}
#| code-fold: true
raw_spend = np.array([1000, 900, 800, 700, 600, 500, 400, 300, 200, 100, 0, 0, 0, 0, 0, 0])

adstock_spend_1 = geometric_adstock(x=raw_spend, alpha=0.20, l_max=8, normalize=True).eval().flatten()
adstock_spend_2 = geometric_adstock(x=raw_spend, alpha=0.50, l_max=8, normalize=True).eval().flatten()
adstock_spend_3 = geometric_adstock(x=raw_spend, alpha=0.80, l_max=8, normalize=True).eval().flatten()

plt.figure(figsize=(10, 6))

plt.plot(raw_spend, marker='o', label='Raw Spend', color='blue')
plt.fill_between(range(len(raw_spend)), 0, raw_spend, color='blue', alpha=0.2)

plt.plot(adstock_spend_1, marker='o', label='Adstock (alpha=0.20)', color='orange')
plt.fill_between(range(len(adstock_spend_1)), 0, adstock_spend_1, color='orange', alpha=0.2)

plt.plot(adstock_spend_2, marker='o', label='Adstock (alpha=0.50)', color='red')
plt.fill_between(range(len(adstock_spend_2)), 0, adstock_spend_2, color='red', alpha=0.2)

plt.plot(adstock_spend_3, marker='o', label='Adstock (alpha=0.80)', color='purple')
plt.fill_between(range(len(adstock_spend_3)), 0, adstock_spend_3, color='purple', alpha=0.2)

plt.xlabel('Weeks')
plt.ylabel('Spend')
plt.title('Geometric Adstock')
plt.legend()
plt.show()

```

In effect geometrick adstock is just a weighted average of media spend in the current period and previous periods where we make an assumption about the maximum duratioon of the carryover effect. As the alpha values decrease the impact of the spend overtime. Generally this is used for re-targeting or campaigns with calls to action. Typically the reason you would set a low alpha value is if you are reasonably confident customers have seen the ad you are just trying to get them back onto your website to do something. The Red Cross doesn't want you to sit there and think about whether you should donate money after a hurricane or earthquake. They want to get you onto the website as quickly as possible to donate resources. 

Inversely if you just get into post-production for a movie and you have a tenative release date 2 months from now running an ad that tries to get you to buy a ticket is not all that helpful. You may want run a campaign where you are trying to get people excited to go to the movies. You would set a higher alpha value to reflect this expected decay. One sticking point about adstock, for me, is that we refer to the alpha parameter, but when we are playing around with the priors to get more realistic looking decays you are going to be changing the beta parameter. So if we wanted to get a similar looking Beta we would do something to this effect. 

```{python}
fig, axe = plt.subplots()
pz.Beta(1,2 ).plot_pdf(legend = "Infinite Memory: Alpha = 1,Beta = 1")
pz.Beta(1,10).plot_pdf(legend = 'Quick Decay:Alpha = 1, Beta = 10')
pz.Beta(1, 5).plot_pdf(legend = 'Slowish Decay:Alpha = 1,Beta = 5')

```


To bring this back to football we can start to imagine how this dynamic arises. During the 2025 season the Rams gained a schematic advantage by using spending more plays in 13 personnel. This allowed the Rams to have heavier bodies on the field while also being able to keep more eligible receivers on the field. The Rams kind of stumbled into this because Puca Nacua sprains his ankle during week 6. We would expect a schematic wrinkle like this to have some boom as teams are caught on the backfoot. However, as the season progresses we expect to see those effects decay a bit. As you the season progresses we would expect explosive plays to regress as teams start to figure out the best ways to match personnel or scheme up ways to stop the Rams. Additionally, as the season progresses more guys tend to get injured so personnel groupings may be less effective. This frees up a defense to invest their resources elsewhere. 

```{python}

```


### Saturation 

Naturally in marketing and football we would expect that as we invest more resources in a particular channel or playoff grouping the returns start to diminish. With 13 personnel you do get some benefits in the run game. TE tend to be better blockers than wide receivers. However, their are only `{python} 11 - (3 + 1 + 6)` receivers on the field. You may have to sacrifice some route concepts that wide receivers would normally run since we wouldn't expect most tight ends to be able to do that and sell the run fake. 

Typically we but a Gamma prior on the saturation parameter. Typically we use Gammas or Inverse Gammas to handle things like rates which are strictly positive, but have no theoretical upper bounds. If you are waiting in line for something you could theoretically wait inline forever, but for the most part there is some practical upperbounds to waitimes. 

Instead of zeroing in on the exact point where the decay happens we are going to place a prior on how quickly we expect diminishing returns start to happen. If we set our saturation to one we expect that for every additional spending unit we would expect the returns to diminish by about a unit. Whereas with a Lambda of 8 we hit that saturation point much quicker. 



```{python}
#| code-fold: true
scaled_spend = np.linspace(start=0.0, stop=1.0, num=100)

saturated_spend_1 = logistic_saturation(x=scaled_spend, lam=1).eval()
saturated_spend_2 = logistic_saturation(x=scaled_spend, lam=2).eval()
saturated_spend_4 = logistic_saturation(x=scaled_spend, lam=4).eval()
saturated_spend_8 = logistic_saturation(x=scaled_spend, lam=8).eval()

plt.figure(figsize=(8, 6))
sns.lineplot(x=scaled_spend, y=saturated_spend_1, label="1")
sns.lineplot(x=scaled_spend, y=saturated_spend_2, label="2")
sns.lineplot(x=scaled_spend, y=saturated_spend_4, label="4")
sns.lineplot(x=scaled_spend, y=saturated_spend_8, label="8")

plt.title('Logistic Saturation')
plt.xlabel('Scaled Marketing Spend')
plt.ylabel('Saturated Marketing Spend')
plt.legend(title='Lambda')
plt.show()
```

We can imagine that as we start to invest more plays into a personnel grouping we are leaving explosive plays out of other personnel groupings on the table. As we invest more plays in the 13 personnel bucket we may not be devouting time to other auxilary heavy personnel packages where with smaller investments we may see more returns. Hypothetically a team may have spent more time game planning for 13 and 11 personnel than some of the other potential personnel packages. So sprinkling in plays out of 21 could yield more explosive plays simply because the defense isn't expecting the Rams to run a ton of 21. 

## Getting Predictions 

So after that long aside lets fit the model. 


```{python}

mmm.fit(X = X, 
        y = y)

```

Then we plot can check how we did by plotting the posterior predictive. 


```{python}

mmm.sample_posterior_predictive(X=X, random_seed = seed)

fig, axes = mmm.plot.posterior_predictive(var=["y_original_scale"], hdi_prob=0.94)
sns.lineplot(
    data=df, x="date_week", y="y", color="black", label="Observed", ax=axes[0][0]
);


```

Importatly the entire bit of MMMs are that we can see the channel contributions over time. Importantly we want to see how impactful our spending campaign in comparision to our baseline. If say we spend 2 million dollars worth of ads across our two channels and we only see a lift of about $500,000 across the two channels. Does the additional spend actual meaningfully shift sales?If you are Coke then probably not but if you are upstart soda company this may be a potentially huge lift. 


```{python}
base_contributions = (
    mmm.idata["posterior"]["intercept_contribution_original_scale"]
    + mmm.idata["posterior"]["control_contribution_original_scale"].sum(dim="control")
    + mmm.idata["posterior"]["yearly_seasonality_contribution_original_scale"]
)

channel_x1 = mmm.idata["posterior"]["channel_contribution_original_scale"].sel(
    channel="x1"
)
channel_x2 = mmm.idata["posterior"]["channel_contribution_original_scale"].sel(
    channel="x2"
)

fig, ax = plt.subplots()

# Stack the contributions
dates = mmm.model.coords["date"]
base_mean = base_contributions.mean(dim=("chain", "draw")).to_numpy()
x1_mean = channel_x1.mean(dim=("chain", "draw")).to_numpy()
x2_mean = channel_x2.mean(dim=("chain", "draw")).to_numpy()

ax.fill_between(dates, 0, base_mean, alpha=0.7, color="gray", label="Base")
ax.fill_between(
    dates,
    base_mean,
    base_mean + x1_mean,
    alpha=0.7,
    color="C0",
    label="Channel x1",
)
ax.fill_between(
    dates,
    base_mean + x1_mean,
    base_mean + x1_mean + x2_mean,
    alpha=0.7,
    color="C1",
    label="Channel x2",
)

# Plot observed
sns.lineplot(data=df, x="date_week", y="y", color="black", label="Observed", ax=ax)

ax.legend(loc="upper left")
ax.set(xlabel="date", ylabel="y")
fig.suptitle("Contribution Breakdown over Time", fontsize=16, fontweight="bold");
```

It looks like we get a reasonable return on spending across both channels. It seems like channel two is giving us a bit more than channel 1. 


## The Problem 

There are a lot of practical friction points with using the `MMM` class from `PyMC-Marketing` and my particular dataset. The big practical impediment is how time works. For sales data you are still going to see some level of sales each of the 52 weeks of the year. Even if one person buys something you are still getting a data point. Practically since we are jut modeling the regular season there are huge gaps in the time-series where nothing is happening. To give you a sense of what this looks like we are just going to bring our NFL data and plot it next to our fake sales data. 


```{python}
#| code-fold: true

from nflreadpy import load_pbp
import matplotlib.dates as mdates

raw_nfl = load_pbp(seasons = range(2017, 2026))

make_explosives = (
    raw_nfl
    .filter(pl.col('season_type') == 'REG')
    .select(
        pl.col('game_id', 'game_date', 'posteam' ,'play_type_nfl', 'receiving_yards', 'rushing_yards')
    )
    .filter(
        (pl.col('play_type_nfl').is_in(['PASS', 'RUSH'])) & 
        (pl.col('posteam') == 'SF')
    )
    .with_columns(
        pl.when(
            (pl.col('play_type_nfl') == 'PASS') & (pl.col('receiving_yards') >= 20)
        )
        .then(1)
        .when(
            (pl.col('play_type_nfl') == 'RUSH') & (pl.col('rushing_yards') >= 10)
        )
        .then(1)
        .otherwise(0)
        .alias('is_explosive')
    )
    .with_columns(
        pl.col('is_explosive').mean().over('game_id').alias('explosive_play_rate'), 
        pl.col('game_date').str.to_date()
    )
    .unique(subset = 'game_id')
    .sort('game_date')
)

fig, axes = plt.subplots(nrows = 2)

axes[0].plot(
    df['date_week'], df['y']
)

axes[0].set_title('Fake Sales Data')

axes[1].plot(
    make_explosives['game_date'], make_explosives['explosive_play_rate']
)
axes[1].set_title('SF Explosive Play Rate')
axes[1].set_ylabel('Explosive Play Rate')
axes[1].set_xlabel('Game Date')
axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))

```

The important thing for now is to focus on the big gaps between seasons. Right now the line is simply trying to interpolate between seasons. This is just a general thing that these plotting libraries will do. 

Why is this a problem? From a theoretical standpoint what happens during the season is important from week to week play callers are going to adapt to their schedule, player availability, how other teams are playing them, and succesful offensive concepts from others. The other important thing to consider is what happens over the course of a coaching career. Successful teams tend to suffer from brain drain. Right now there are 10 NFL head coaches that were either on Kyle Shanhan or Sean McVay's staff at some point. This is not including current offensive coordinators like Mike McDaniel or Bobby Slowik who had previously worked on their staffs. As a coaching staff starts to experience brain drain we may start to see less returns in the area that they oversaw. 

Additionally, as more of these assistants get hired to other teams more teams are exposed to these concepts. Teams have to adapt by not only coming up with different ways to stop these offenses, but invest money and draft picks to get the personnel to maximize these defensive schemes. As the league adapts to the offensive trends foundational concepts to the offense become less effective. This forces other parts of the tree to adapt or be fired. 


Practicallly we run into other problems. The MMM that comes with `PyMC-Marketing` is really well thought out and put together class method using `PyMC`'s model builder class. They have a ton of functionality that you would if you were on an analytics team in a marketing role. As you would imagine one of the things that the developers enforce is that you have to pass a valid date to the `MMM` class. You could hack it a little bit but we are making up data to make it a little more convenient for yourself. 


# Building A Football MMM

If you skipped the non-football section I will give you a brief summary. One of the main motivations to use a MMM is that there are some similarities that make this model interesting. The first being that we expect the returns of new personnel formations and plays out of these formations to decay quickly. The second being that there is a theoretical saturation point where you are just trying to squeeze blood from a stone. 