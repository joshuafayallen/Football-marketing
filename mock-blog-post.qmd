---
title: "What Marketing Taught Me About Football"
subtitle: "And What Football Taught Me about Marketing"
format: html
execute: 
  warning: false
  message: false
  knitr:
    opts_chunk:
        fig_pos: "H"
---

## Introduction 

The rise of McVay's use of 13 personnel made me think of an old [*Athletic Football Show*](https://youtu.be/93RzplSc3mM?si=_2p7P7xEF0g43bxu&t=3395) episode where they pointed out the use of auxiliary runs to get to explosives. This got me thinking about how can we model the returns on NFL plays.  A major talking point in the 2025 season was Sean McVay's usage of 13 personnel to force another linebacker onto the field. The benefit of this is that they offense has heavier bodies on the field to help the run game but you are keeping more eligible receivers on the field. Since Kyle Shanahan and Sean McVay started as head coaches 8 years ago they have been two of the best play callers and play designers in the game. One way that their offenses are so good is that they find different ways to generate explosive plays. Typically explosive plays are defined as runs of ten plus yards or pass plays of twenty plus yards. Effectively you are gaining the equivalent of one plus first down on one play shortening the field and reducing the number of plays you have to run, or a quick touchdown. 

There are a variety of ways that Kyle Shanahan and Sean McVay try to manipulate defenses. The most obvious ways that they do this is the use of pre-snap motion. Pre-snap motion can help the QB check into the right play by tipping whether a defense is in man or zone. Motion directly before the snap forces the defense to adjust run and pass assignments on the fly creating and getting offensive players up to top speed before they start the play. 


Another way that Kyle and Sean  try to manufacture explosive plays is by using personnel packages in creative ways. Each personnel package has certain advantage but, generally, the idea is that you want a certain defenseive personnel grouping on the field and leverage their weaknesses. To make sure we are on the same page on what different personnel packages mean I have broken down the most common ones in @tbl-personnel. Conventionally personnel pacakges are broken down by the number of RBs on the field and the number of TEs on the field.

```{python}
#| echo: false
#| label: tbl-personnel 
#| tbl-cap: "Common Personnel Packages"
import polars as pl 
import polars.selectors as cs
from great_tables import GT

te_df = pl.DataFrame(
    {'Number of TEs': [1,2,3]}
)

rb_df = pl.DataFrame(
    {'Number of RBs': [1,2,3]}
)
personnel_df = rb_df.join(te_df, how = 'cross')
personnel_df = (rb_df
    .join(te_df, how = 'cross')
    .with_columns(
        pl.concat_str([
            'Number of RBs',
            'Number of TEs'
        ]).alias('Personnel Package'))
    .with_columns(
        pl.col('Personnel Package').str.to_integer().alias('p')
    )   
        .filter(pl.col('p') <= 22)
        .drop('p')
)

GT(personnel_df)

```


Since the days of the big neckroll the league has shifted. Due to changes in the rules passing has massively upticked. As a result defenses have tried to combat this schematically by playing with less defenders in the box, more defensive backs, and more complex pass coverages. Early on Kyle and Sean used a lot of under-center play action so defenses would simply change the picture on the quarterback when their backs were turned. To counter this Kyle has used a lot of 21 personnel to force a conflict between defending the run with lighter bodies or defending the pass with heavier bodies. 

With all the talk of 13 personnel this year and ton of talk about pre-snap motion from the past few years I got kind of curious. What are the returns of some of these schematic trends and how can we model it? This is an interesting question for a variety of reasons. Should we focus on the design and mechanics of the play or should we focus more on what the bodies on the field get you? Well that kind of depends on a lot of things and as was the case for me what data you can access. Since I only have participation data I decided to look at explosive plays as a function of personnel grouping. 

## How Can We Model This? 

There are lots of ways we can model this but because play callers have these different personel packages that they can use my mind immediately went to Media Mixed Modeling(MMMs). What is an MMM? Lets say you are in a marketing department at a large electronics company with a 1 million dollar budget to spend on advertising. You spend that million dollars across various marketing channels whether this search optimization, TV, email marketing, or influencer marketing.^[There are many more places and ways to advertise] You then see that sales increased that month, but you want to know which channel was responsible for this spike so you can invest your money more wisely. In the early digital marketing age companies relied on multi-touch attribution models. The general idea of this was that if a consumer saw an ad on Facebook clicked on it and got the product. You could track those clicks back to consumer and then estimate a model to better undertand where to put your money. 

However, around 2020 as a response to privacy concerns, laws passed by various governing bodies, and IOS14 these models started to break. More people started using ad blockers, Apple made tracking these attribution points much harder, and some people started using browsers that restricted access to what companies could track. In short, following the customer behavior got a lot harder. So marketing analytics turned to good old fashion experiments and MMMs. The general idea of an MMM is that weekly conversions, sales, revenue, etc are a function of

$$
Sales_t = \alpha + \overbrace{\sum^{m}_{m=1}\beta_{m}F(x^{*}_{t,m})}^{\text{Impact of marketing}} + \overbrace{\sum^{c}_{c=1} \gamma_{c} Z_{t,c}}^{\text{Impact of Controls}} + \varepsilon_{t}
$$


We can then go ahead and measure the relative contribution of each channel and forecast forward on where we should spend our money and about how much money we should spend on that channel. As you may imagine there are lots of gnarly time dynamics in marketing that you have to deal with. As an electronics company executive you would anticipate that every year in late October to early November sales effectively just stop as people wait for Black Friday sales. You also anticipate that there is only so many ads you can run on the same customer base before they stop caring. 

Snapping back to football this made me think of personnel groupings as channels. You more or less have a fixed number of plays per game which would track with a marketing budget. The play design may be more akin to a specific ads on each of these platforms to adapt to the user base and platform specific rules.^[Admittedly this is a pretty big simplifying assumption.] In addition, both marketing and football deal with really noisy data that try to capture complex behavioral dynamics and you do not have a ton of data to work with generally. Based on this idea I decided to try and build an MMM to understand how play-callers should invest their play budget. Perhaps just as importantly this felt like a fun opportunity to figure out how to build an MMM. 


# Building a Regular MMM

## Making The Data 

For those only interested in the Football stuff you should just skip to the next section. For those sticking around for I am just working through the example provided by [PyMC-Marketing](https://www.pymc-marketing.io/en/stable/notebooks/mmm/mmm_example.html).^[Where this is going to differ is I am going to use Polars instead of Pandas.] We are going to generate some synthetic data for a made up company at the weekly level over two years. This roughly equates to `{python} 52*2` weeks of data.

```{python}
#| echo: true
#| code-fold: true
#| label: mod-import

import arviz as az 
import matplotlib.pyplot as plt
import numpy as np 
import pandas as pd
import preliz as pz
import pymc as pm
import seaborn as sns
from pymc_extras.prior import Prior
from pymc_marketing.mmm import GeometricAdstock, LogisticSaturation
from pymc_marketing.mmm.multidimensional import MMM
from pymc_marketing.mmm.transformers import geometric_adstock, logistic_saturation
from datetime import date
# seed from random.org

seed = 39233615
rng: np.random.Generator = np.random.default_rng(seed = seed)

min_date = date(2018, 4,1)
max_date= date(2021, 9, 1)
plt.rcParams['figure.figsize'] = [12,7]

```

### Media Costs 

First we are going to generate two years worth of data 


```{python}
#| label: make-years-data
df = (pl.DataFrame({
    'date_week': pl.date_range(min_date, max_date, "1d", eager = True)}
    )
    .filter(pl.col("date_week").dt.weekday() == 1)
    .with_columns(
        pl.col('date_week').dt.year().alias('year'),
        pl.col('date_week').dt.month().alias('month'),
        pl.col('date_week').dt.ordinal_day().alias('day_of_year')
    )
)


```

Next we are going to generate synthetic data for two channels. Which are going to be a bit different based on carryover and the saturation parameters. Roughly the costs for the each of the channels look like this. 


```{python}
#| code-fold: true
#| label: make-raw-spend
n = df.height
x1 = rng.uniform(low = 0.0, high = 1.0, size = n)
x2 = rng.uniform(low = 0.0, high = 1.0, size = n)


df = (
    df
    .with_columns(
        x1_raw = pl.Series(x1), 
        x2_raw = pl.Series(x2)
    )
    .with_columns(
        pl.when(pl.col('x1_raw') > 0.9)
        .then(pl.col('x1_raw'))
        .otherwise((pl.col('x1_raw')/2))
        .alias('x1'),
        pl.when(pl.col('x2_raw') >0.8)
        .then(pl.col("x2_raw"))
        .otherwise(0)
        .alias('x2')
    )
    .drop(['x1_raw', 'x2_raw'])
)

df.columns

long_data = df.unpivot(on = ['x1','x2'], index = 'date_week')

fig, ax = plt.subplots()
sns.lineplot(data = long_data, x = 'date_week', y = 'value', hue = 'variable', alpha = 0.5)

```

Next we are going to pass each of these through our adstock function. In media marketing our adstock is effectively our carryover effect. If you see an ad one day how long is that going to stay with you? 


```{python}
#| code-fold: True
#| label: make-adstock
alpha1: float = 0.4 
alpha2: float = 0.2

df = (
    df
    .with_columns(
        pl.col('x1')
        .map_batches(
            lambda s: geometric_adstock(
                x = s.to_numpy(),
                alpha = alpha1,
                l_max = 8,
                normalize=True
            ).eval().flatten(), 
            return_dtype=pl.Float64
        ).alias('x1_adstock'),
        pl.col('x2')
        .map_batches(
            lambda s: geometric_adstock(
                x = s.to_numpy(),
                alpha = alpha2,
                l_max = 8,
                normalize=True
            ).eval().flatten(), 
            return_dtype=pl.Float64
        ).alias('x2_adstock')
    )
)


```

The next thing we are going to do is add our concentration effects or how much money we spending in each chanel. 


```{python}
#| code-fold: true
#| label: make-saturation
lam1: float = 4.0
lam2: float = 3.0

df = (
    df
    .with_columns(
        pl.col('x1_adstock')
        .map_batches(
            lambda s: 
            logistic_saturation(
                x = s.to_numpy(),
                lam = lam1
            ).eval(), 
            return_dtype=pl.Float64
        ).alias('x1_saturated_adstock'), 
        pl.col('x2_adstock')
        .map_batches(
            lambda s: 
            logistic_saturation(
                x = s.to_numpy(),
                lam = lam1
            ).eval(), 
            return_dtype=pl.Float64
        ).alias('x2_saturated_adstock')
    )
)


```

Then as always we are going to plot this to make sure everything is good. 


```{python}
long_effects = df.unpivot(on = cs.starts_with('x'), index = 'date_week')

g = sns.FacetGrid(data= long_effects, col = 'variable', col_wrap = 2)
g.map(sns.lineplot, 'date_week', 'value')

```

This is looks like the costs are pretty realistic. 


### Trend and Seasonal Parts 

Now we are going to add some seasonality 



```{python}
#| code-fold: true
import math 
df = df.with_columns(
    trend=(
        pl.linear_space(0.0, 50.0, n)  # sequence 0..50 with n samples
        .add(10.0)
        .pow(1.0 / 4.0)
        .sub(1.0)
    ),
    cs=(
        -(2.0 * 2.0 * math.pi * pl.col("day_of_year") / 365.5).sin()
    ),
    cc=(
        (1.0 * 2.0 * math.pi * pl.col("day_of_year") / 365.5).cos()
    ),
).with_columns(
    seasonality=0.5 * (pl.col("cs") + pl.col("cc"))
)

fig, ax = plt.subplots()
sns.lineplot(x="date_week", y="trend", color="C2", label="trend", data=df, ax=ax)
sns.lineplot(
    x="date_week", y="seasonality", color="C3", label="seasonality", data=df, ax=ax
)
ax.legend(loc="upper left")
ax.set(xlabel="date", ylabel=None)
ax.set_title("Trend & Seasonality Components", fontsize=18, fontweight="bold");
```


### Making Controls 

Finally, we will just add our dependent variable and our controls 



```{python}
#| code-fold: true
epsilon = rng.normal(loc = 0.0, scale = 0.25, size = n)
amplitude = 1 
beta_1 = 3.0
beta_2 = 2.0
betas = [beta_1, beta_2]


df = (
    df
    .with_columns(
        pl.lit(2.0).alias('intercept'),
        pl.Series(epsilon).alias('epsilon'), 
        pl.lit(beta_1).alias('beta_1'), 
        pl.lit(beta_2).alias('beta_2'), 
        event_1=(pl.col("date_week") == date(2019, 5,13)).cast(pl.Float64),
        event_2=(pl.col("date_week") == date(2020, 9, 14)).cast(pl.Float64),
    )
    .with_columns(

    )
    .with_columns(
        (
            pl.col('intercept')
            + pl.col('trend')
            + pl.col("seasonality")
            + pl.lit(1.5) * pl.col('event_1')
            + pl.lit(2.5) * pl.col('event_2')
            + pl.col('beta_1') * pl.col('x1_saturated_adstock')
            + pl.col('beta_2') * pl.col('x2_saturated_adstock')
            + pl.col('epsilon')
        ).alias('y')
    )
)

fig, ax = plt.subplots()
sns.lineplot(x="date_week", y="y", color="black", data=df, ax=ax)
ax.set(xlabel="date", ylabel="y (thousands)")


```

[We can now go ahead and look at the ground truth](https://www.pymc-marketing.io/en/stable/notebooks/mmm/mmm_example.html), but that is an excercise left for the reader. 


## Building The Regular MMM

For whatever reason I found that working with PyMC is just easier to do in pandas so we are going to throw it over to Pandas. 


```{python}

df_pd = df.to_pandas()
columns_to_keep = [
    "date_week",
    "y",
    "x1",
    "x2",
    "event_1",
    "event_2",
    "day_of_year",
]

mod_df = df_pd[columns_to_keep]

mod_df['t'] = range(n)

```


The whole stick of using Bayesian stats is to set appropriate priors. We know at least a few things 

- Channel Contributions are positive and vary. So we are going to set a 1 sigma per channel. How much they vary is an open question. The wider the prior the more area we give the model to explore. This has some pros and cons.
- We kind of expect channels where we spend the most to have more attributed sales. We wouldn't advertise a product targeted for an older audience on TikTok as an example. 


The `MMM` class in `PyMC-Marketing` passes parts of your data through the `MaxABs` scaler in `scikit-learn`. So we have to express our priors in on this scale. So what would normally be a weakly informative prior may actually be super informative. 


```{python}

from sklearn.preprocessing import MaxAbsScaler
scaler = MaxAbsScaler()

examp = (
    df
    .with_columns(
        pl.col('x1_saturated_adstock')
        .map_batches(
            lambda s: scaler.fit_transform(s.to_numpy().reshape(-1,1)).ravel(),
            return_dtype = pl.Float64
        ).alias('scaled_version')
    )
)

long = examp.unpivot(on = ['x1_saturated_adstock', 'scaled_version'], index= 'date_week')

fig,ax = plt.subplots()
sns.scatterplot(data = long, x = 'date_week', y= 'value', hue = 'variable')


```


We are going to define our sigma priors as a share of what we spend on them. 


```{python}

spend_per_channel = mod_df[['x1', 'x2']].sum(axis = 0)

spend_share =  spend_per_channel/spend_per_channel.sum()

sigma_prior = 2 * spend_share.to_numpy()

X = mod_df.drop('y', axis = 1)

y = mod_df['y']
```


Next we are just going to define some priors. 


```{python}
my_priors= {
    "intercept": Prior("Normal", mu=0.5, sigma=0.2),
    "adstock_alpha": Prior("Beta", alpha=1, beta=3, dims="channel"),
    "saturation_beta": Prior("HalfNormal", sigma=sigma_prior, dims="channel"),
    "saturation_lam": Prior("Gamma", alpha=3, beta=1, dims="channel"),
    "gamma_control": Prior("Normal", mu=0, sigma=0.05, dims="control"),
    "gamma_fourier": Prior("Laplace", mu=0, b=0.2, dims="fourier_mode"),
    "likelihood": Prior("Normal", sigma=Prior("HalfNormal", sigma=6)),
}

samp_config = {'progressbar': True, 'random_seed': seed, 'nuts_sampler': 'numpyro'}

mmm = MMM(
    model_config=my_priors,
    sampler_config=samp_config,
    date_column="date_week",
    adstock=GeometricAdstock(l_max=8),
    saturation=LogisticSaturation(),
    channel_columns=["x1", "x2"],
    control_columns=["event_1", "event_2", "t"],
    yearly_seasonality=2,
    target_column = 'y'
)
mmm.build_model(X,y)

mmm.add_original_scale_contribution_variable(
    var=[
        "channel_contribution",
        "control_contribution",
        "intercept_contribution",
        "yearly_seasonality_contribution",
        "y",
    ]
)


```


As always we should look a the prior predictive to make sure things look good 


```{python}

mmm.sample_prior_predictive(X,y)

fig,axes = mmm.plot.prior_predictive()

```


Since we have the ability to build a model where we know the true DGP we know these priors are pretty weakly informative. Basically all this means is that we want to give the model a large enough space to explore, but not so big that we let the model go from $-\infty$ to $\infty$

## What are some of these parameters?

So the entire crux of MMMs is that we don't expect ads: to work instantly, every additional ad to increase sales, and the effect of an ad to persist that much. Thee multiple avenues to see growth in the outcome of interest was what initially got me thinking about using these models. But when I started digging more into these models these time dynamics became what got me really excited to see how this could work. In this section I am going to borrow heavily from some excellent blog posts by [Ryan O'Sullivan](https://towardsdatascience.com/mastering-marketing-mix-modelling-in-python-7bbfe31360f9/?source=post_page-----49dce1a5b33d---------------------------------------)


### Adstock

Adstock really just refers to the decay rate. So if I run an advertisement how quickly do those effects diminsh overtime. Adstock is typically conceptualized as a ratio where  . Typically adstock is constrained to be between zero and 1 so the Beta distribution is used more often than not. A vagueish prior would be a $adstock \sim \beta(\alpha = 1, \beta = 3)$ or something that looks like this 

```{python}
fig, axe = plt.subplots()
pz.Beta(alpha = 1, beta = 3).plot_pdf()

```


To build an intuition on what that may mean practically we can simulate some spend


```{python}
#| code-fold: true
raw_spend = np.array([1000, 900, 800, 700, 600, 500, 400, 300, 200, 100, 0, 0, 0, 0, 0, 0])

adstock_spend_1 = geometric_adstock(x=raw_spend, alpha=0.20, l_max=8, normalize=True).eval().flatten()
adstock_spend_2 = geometric_adstock(x=raw_spend, alpha=0.50, l_max=8, normalize=True).eval().flatten()
adstock_spend_3 = geometric_adstock(x=raw_spend, alpha=0.80, l_max=8, normalize=True).eval().flatten()

plt.figure(figsize=(10, 6))

plt.plot(raw_spend, marker='o', label='Raw Spend', color='blue')
plt.fill_between(range(len(raw_spend)), 0, raw_spend, color='blue', alpha=0.2)

plt.plot(adstock_spend_1, marker='o', label='Adstock (alpha=0.20)', color='orange')
plt.fill_between(range(len(adstock_spend_1)), 0, adstock_spend_1, color='orange', alpha=0.2)

plt.plot(adstock_spend_2, marker='o', label='Adstock (alpha=0.50)', color='red')
plt.fill_between(range(len(adstock_spend_2)), 0, adstock_spend_2, color='red', alpha=0.2)

plt.plot(adstock_spend_3, marker='o', label='Adstock (alpha=0.80)', color='purple')
plt.fill_between(range(len(adstock_spend_3)), 0, adstock_spend_3, color='purple', alpha=0.2)

plt.xlabel('Weeks')
plt.ylabel('Spend')
plt.title('Geometric Adstock')
plt.legend()
plt.show()

```

In effect geometrick adstock is just a weighted average of media spend in the current period and previous periods where we make an assumption about the maximum duratioon of the carryover effect. As the alpha values decrease the impact of the spend overtime. Generally this is used for re-targeting or campaigns with calls to action. Typically the reason you would set a low alpha value is if you are reasonably confident customers have seen the ad you are just trying to get them back onto your website to do something. The Red Cross doesn't want you to sit there and think about whether you should donate money after a hurricane or earthquake. They want to get you onto the website as quickly as possible to donate resources. 

Inversely if you just get into post-production for a movie and you have a tenative release date 2 months from now running an ad that tries to get you to buy a ticket is not all that helpful. You may want run a campaign where you are trying to get people excited to go to the movies. You would set a higher alpha value to reflect this expected decay. One sticking point about adstock, for me, is that we refer to the alpha parameter, but when we are playing around with the priors to get more realistic looking decays you are going to be changing the beta parameter. So if we wanted to get a similar looking Beta we would do something to this effect. 

```{python}
fig, axe = plt.subplots()
pz.Beta(1,2 ).plot_pdf(legend = "Infinite Memory: Alpha = 1,Beta = 1")
pz.Beta(1,10).plot_pdf(legend = 'Quick Decay:Alpha = 1, Beta = 10')
pz.Beta(1, 5).plot_pdf(legend = 'Slowish Decay:Alpha = 1,Beta = 5')

```


To bring this back to football we can start to imagine how this dynamic arises. During the 2025 season the Rams gained a schematic advantage by using spending more plays in 13 personnel. This allowed the Rams to have heavier bodies on the field while also being able to keep more eligible receivers on the field. The Rams kind of stumbled into this because Puca Nacua sprains his ankle during week 6. We would expect a schematic wrinkle like this to have some boom as teams are caught on the backfoot. However, as the season progresses we expect to see those effects decay a bit. As you the season progresses we would expect explosive plays to regress as teams start to figure out the best ways to match personnel or scheme up ways to stop the Rams. Additionally, as the season progresses more guys tend to get injured so personnel groupings may be less effective. This frees up a defense to invest their resources elsewhere. 




### Saturation 

Naturally in marketing and football we would expect that as we invest more resources in a particular channel or playoff grouping the returns start to diminish. With 13 personnel you do get some benefits in the run game. TE tend to be better blockers than wide receivers. However, their are only `{python} 11 - (3 + 1 + 6)` receivers on the field. You may have to sacrifice some route concepts that wide receivers would normally run since we wouldn't expect most tight ends to be able to do that and sell the run fake. 

Typically we but a Gamma prior on the saturation parameter. Typically we use Gammas or Inverse Gammas to handle things like rates which are strictly positive, but have no theoretical upper bounds. If you are waiting in line for something you could theoretically wait inline forever, but for the most part there is some practical upperbounds to waitimes. 

Instead of zeroing in on the exact point where the decay happens we are going to place a prior on how quickly we expect diminishing returns start to happen. If we set our saturation to one we expect that for every additional spending unit we would expect the returns to diminish by about a unit. Whereas with a Lambda of 8 we hit that saturation point much quicker. 



```{python}
#| code-fold: true
scaled_spend = np.linspace(start=0.0, stop=1.0, num=100)

saturated_spend_1 = logistic_saturation(x=scaled_spend, lam=1).eval()
saturated_spend_2 = logistic_saturation(x=scaled_spend, lam=2).eval()
saturated_spend_4 = logistic_saturation(x=scaled_spend, lam=4).eval()
saturated_spend_8 = logistic_saturation(x=scaled_spend, lam=8).eval()

plt.figure(figsize=(8, 6))
sns.lineplot(x=scaled_spend, y=saturated_spend_1, label="1")
sns.lineplot(x=scaled_spend, y=saturated_spend_2, label="2")
sns.lineplot(x=scaled_spend, y=saturated_spend_4, label="4")
sns.lineplot(x=scaled_spend, y=saturated_spend_8, label="8")

plt.title('Logistic Saturation')
plt.xlabel('Scaled Marketing Spend')
plt.ylabel('Saturated Marketing Spend')
plt.legend(title='Lambda')
plt.show()
```

We can imagine that as we start to invest more plays into a personnel grouping we are leaving explosive plays out of other personnel groupings on the table. As we invest more plays in the 13 personnel bucket we may not be devouting time to other auxilary heavy personnel packages where with smaller investments we may see more returns. Hypothetically a team may have spent more time game planning for 13 and 11 personnel than some of the other potential personnel packages. So sprinkling in plays out of 21 could yield more explosive plays simply because the defense isn't expecting the Rams to run a ton of 21. 

## Getting Predictions 

So after that long aside lets fit the model. 


```{python}

mmm.fit(X = X, 
        y = y)

```

Then we plot can check how we did by plotting the posterior predictive. 


```{python}

mmm.sample_posterior_predictive(X=X, random_seed = seed)

fig, axes = mmm.plot.posterior_predictive(var=["y_original_scale"], hdi_prob=0.94)
sns.lineplot(
    data=df, x="date_week", y="y", color="black", label="Observed", ax=axes[0][0]
);


```

Importatly the entire bit of MMMs are that we can see the channel contributions over time. Importantly we want to see how impactful our spending campaign in comparision to our baseline. If say we spend 2 million dollars worth of ads across our two channels and we only see a lift of about $500,000 across the two channels. Does the additional spend actual meaningfully shift sales?If you are Coke then probably not but if you are upstart soda company this may be a potentially huge lift. 


```{python}
#| code-fold: true
base_contributions = (
    mmm.idata["posterior"]["intercept_contribution_original_scale"]
    + mmm.idata["posterior"]["control_contribution_original_scale"].sum(dim="control")
    + mmm.idata["posterior"]["yearly_seasonality_contribution_original_scale"]
)

channel_x1 = mmm.idata["posterior"]["channel_contribution_original_scale"].sel(
    channel="x1"
)
channel_x2 = mmm.idata["posterior"]["channel_contribution_original_scale"].sel(
    channel="x2"
)

fig, ax = plt.subplots()

# Stack the contributions
dates = mmm.model.coords["date"]
base_mean = base_contributions.mean(dim=("chain", "draw")).to_numpy()
x1_mean = channel_x1.mean(dim=("chain", "draw")).to_numpy()
x2_mean = channel_x2.mean(dim=("chain", "draw")).to_numpy()

ax.fill_between(dates, 0, base_mean, alpha=0.7, color="gray", label="Base")
ax.fill_between(
    dates,
    base_mean,
    base_mean + x1_mean,
    alpha=0.7,
    color="C0",
    label="Channel x1",
)
ax.fill_between(
    dates,
    base_mean + x1_mean,
    base_mean + x1_mean + x2_mean,
    alpha=0.7,
    color="C1",
    label="Channel x2",
)

# Plot observed
sns.lineplot(data=df, x="date_week", y="y", color="black", label="Observed", ax=ax)

ax.legend(loc="upper left")
ax.set(xlabel="date", ylabel="y")
fig.suptitle("Contribution Breakdown over Time", fontsize=16, fontweight="bold");
```

It looks like we get a reasonable return on spending across both channels. It seems like channel two is giving us a bit more than channel 1. 


## The Problem 

There are a lot of practical friction points with using the `MMM` class from `PyMC-Marketing` and my particular dataset. The big practical impediment is how time works. For sales data you are still going to see some level of sales each of the 52 weeks of the year. Even if one person buys something you are still getting a data point. Practically since we are jut modeling the regular season there are huge gaps in the time-series where nothing is happening. To give you a sense of what this looks like we are just going to bring our NFL data and plot it next to our fake sales data. 


```{python}
#| code-fold: true

from nflreadpy import load_pbp
import matplotlib.dates as mdates

raw_nfl = pl.read_parquet('blog-post-data/pbp-data.parquet')

make_explosives = (
    raw_nfl
    .filter(pl.col('season_type') == 'REG')
    .select(
        pl.col('game_id', 'game_date', 'posteam' ,'play_type_nfl', 'receiving_yards', 'rushing_yards')
    )
    .filter(
        (pl.col('play_type_nfl').is_in(['PASS', 'RUSH'])) & 
        (pl.col('posteam') == 'SF')
    )
    .with_columns(
        pl.when(
            (pl.col('play_type_nfl') == 'PASS') & (pl.col('receiving_yards') >= 20)
        )
        .then(1)
        .when(
            (pl.col('play_type_nfl') == 'RUSH') & (pl.col('rushing_yards') >= 10)
        )
        .then(1)
        .otherwise(0)
        .alias('is_explosive')
    )
    .with_columns(
        pl.col('is_explosive').mean().over('game_id').alias('explosive_play_rate'), 
        pl.col('game_date').str.to_date()
    )
    .unique(subset = 'game_id')
    .sort('game_date')
)

fig, axes = plt.subplots(nrows = 2)

axes[0].plot(
    df['date_week'], df['y']
)

axes[0].set_title('Fake Sales Data')

axes[1].plot(
    make_explosives['game_date'], make_explosives['explosive_play_rate']
)
axes[1].set_title('SF Explosive Play Rate')
axes[1].set_ylabel('Explosive Play Rate')
axes[1].set_xlabel('Game Date')
axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))

```

The important thing for now is to focus on the big gaps between seasons. Right now the line is simply trying to interpolate between seasons. This is just a general thing that these plotting libraries will do. 

Why is this a problem? From a theoretical standpoint what happens during the season is important from week to week play callers are going to adapt to their schedule, player availability, how other teams are playing them, and succesful offensive concepts from others. The other important is does not capture what we care about between seasons, their tenure length does. Successful teams tend to suffer from brain drain. Right now there are 10 NFL head coaches that were either on Kyle Shanhan or Sean McVay's staff at some point. This is not including current offensive coordinators like Mike McDaniel or Bobby Slowik who had previously worked on their staffs. As a coaching staff starts to experience brain drain we may start to see less returns in the area that they oversaw. 

There is a practical tension on the implementation side. Marketing campaigns are naturally going to be on calendar time without huge gaps in the marketing campaign. Naturally, the implentation of and MMM by `PyMC` is going to make the sane decision to use valid date times. We could theoretically use the game date in the classs method but that results a in weird looking trends. Just passing an integer to the class method results in  errors. 

# Football Stuff

If you skipped the non-football section I will give you a brief summary. One of the main motivations to use a MMM is that there are some similarities that make this model interesting. The first being that we expect the returns of new personnel formations and plays out of these formations to decay quickly. The second being that there is a theoretical saturation point where you are just trying to squeeze blood from a stone. In my own head that sounds a lot like the dynamics in a Football season. In this section I am going to go through the full model building process and the Bayesian workflow that led me to the final model. Then, I promise, I will get to the insights about play callers. 


## Building the Football Data

For the entire rest of the blog post I rely on data provided by the `nflverse` team. Specifically we are going to look at the participation data from 2017-2024. Prior to 2023 this data were provided by NFL NGS while post 2023 the data are provided by FTN.^[You may know them as the company behind DVOA] Specifically we are interested in the offensive formation columns and defensive personnel. To give you a sense of what this looks like I provide an example for the first 5 plays of the Titans vs Bears game in the 2024 season.


```{python}
#| echo: false

from nflreadpy import load_participation

examp = (
    load_participation()
    .select(pl.col('nflverse_game_id','offense_personnel', 'defenders_in_box'))
    .drop('nflverse_game_id')
    .head(n = 5)
)

```

The first order of business is to only include offensive plays this is pretty easy since there is a little flag to indicate whether it was a run, pass, special teams play etc. As far as regexs go this was a pretty simple one, but there is one tripping point. Typically we count the fullback as a running packing in these personnel packages, but in a much appreciated effort to keep the raw data intact they are recorded by their roster position. So for the 49ers who typically run 21 personnel you get this.   


```{python}

examp2 = (
    load_participation()
    .filter((pl.col('defenders_in_box') != 0) & (pl.col('possession_team') == 'SF'))
    .select(pl.col("offense_personnel"))
    .head(n = 5)
    .with_columns(
        pl.col('offense_personnel').str.extract("(\\d{1} RB)").str.replace('RB','').str.strip_chars_end().alias('n_rbs'),
        pl.col('offense_personnel').str.extract("(\\d{1} TE)").str.replace('TE','').str.strip_chars_end().alias('n_tes'), 
        pl.col('offense_personnel').str.extract("(\\d{1} FB)").str.replace('FB','').str.strip_chars_end().alias('n_fb'),
    )
    .with_columns(
        pl.concat_str([
            pl.col('n_rbs'), 
            pl.col('n_tes')
        ]).alias('personnel_grouping')

    )
)

GT(examp2.drop(['n_rbs', 'n_tes', 'n_fb']))


```

The simple trick to this is really just to add the RBs and Fullbacks together then convert those back to strings. 


```{python}

fixed = (
    examp2 
    .with_columns(
        (pl.col('n_rbs').cast(pl.Int64).fill_null(0) + pl.col('n_fb').cast(pl.Int64).fill_null(0)).alias("total_rbs")
    )
    .with_columns(
        pl.concat_str(
            [   pl.lit('personnel_'),
                pl.col('total_rbs').cast(pl.String),
                pl.col('n_tes')
                
            ]
        ).alias('personnel_grouping_fixed')
    ).drop(['n_rbs', 'n_tes', 'n_fb', 'total_rbs'])
)

GT(fixed)

```

So now these values look more sane than the ones that I initially came up with. The next step is to build the spend share variables and define the relevant time dimensions for between season. I did this in R just because the `nflreadr` version of the package lets you automatically merge the play-by-play data. For the playcaller data I am using [Sam Hoppen's data](https://github.com/samhoppen/NFL_public/blob/main/data/all_playcallers.csv). I just take the minimum for each play caller in the dataset and then subtract it from the current season to get their tenure. This approach does have a drawback just because we would expect that there is a bigger jump for a new playcaller from the start of their first season to the end of their first season.^[Very much open to suggestions on a better way to do this] Whereas a more experienced play caller it may not actually matter that much by the time they hit year four or five. My "ad spend" is just the number of plays that each playcaller makes out of each personell grouping.


```{r}
#| eval: false
#| code-fold: true

library(nflreadr)
library(tidyverse)

df = load_participation(
  seasons = c(2017:2025),
  include_pbp = TRUE
) |>
  filter(play_type_nfl %in% c('PASS', 'RUSH'), season_type == 'REG')


play_callers = read_csv(
  "https://raw.githubusercontent.com/samhoppen/NFL_public/refs/heads/main/data/all_playcallers.csv"
) |>
  select(
    season,
    team,
    week,
    game_id,
    off_play_caller
  ) |>
  mutate(
    play_caller_mins = min(season),
    .by = off_play_caller
  ) |>
  mutate(play_caller_tenure = season - play_caller_mins) |>
  filter(
    off_play_caller %in%
      c(
        'Kyle Shanahan',
        'Sean McVay',
        'Ben Johnson',
        'Josh McDaniels',
        "Matt LaFleur",
        'Mike McDaniel',
        "Kevin O'Conell",
        'Andy Reid',
        'Liam Coen'
      )
  ) |>
  filter_out(season == 2026)


add_play_callers = df |>
  left_join(
    play_callers,
    join_by(nflverse_game_id == game_id, season, week, possession_team == team)
  ) 

make_snap_shares = add_play_callers |>
  mutate(
    n_tes = str_extract(offense_personnel, "\\d+ TE"),
    n_rbs = str_extract(offense_personnel, "\\d+ RB"),
    n_fbs = str_extract(offense_personnel, "\\d+ FB"),
    across(c(n_tes, n_rbs, n_fbs), \(x) ifelse(is.na(x), 0, x)),
    across(c(n_tes, n_rbs, n_fbs), \(x) str_remove_all(x, "TE|RB|FB| ")),
    across(c(n_rbs, n_fbs), \(x) as.numeric(x)),
    n_rbs = as.character(n_rbs + n_fbs),
    personnel_grouping = as.factor(str_glue("personnel_{n_rbs}{n_tes}"))
  ) |>
  mutate(total_snaps = n(), .by = c(off_play_caller, nflverse_game_id)) |>
  mutate(
    snaps_by_personnel = n(),
    .by = c(off_play_caller, nflverse_game_id, personnel_grouping)
  ) |>
  mutate(share = (snaps_by_personnel / total_snaps)) |>
  group_by(personnel_grouping, off_play_caller) |>
  distinct(nflverse_game_id, .keep_all = TRUE) |>
  ungroup() |>
  pivot_wider(
    names_from = personnel_grouping,
    values_from = share,
    values_fill = 0,
    id_cols = c(nflverse_game_id, off_play_caller)
  )


make_features = add_play_callers |>
  mutate(
    is_explosive = case_when(
      play_type_nfl == 'RUN' & rushing_yards >= 10 ~ 1,
      play_type_nfl == 'PASS' & receiving_yards >= 20 ~ 1,
      .default = 0
    ),
    is_pass = ifelse(play_type_nfl == 'PASS', 1, 0)
  ) |>
  group_by(season, nflverse_game_id, off_play_caller) |>
  summarise(
    success_rate = mean(success, na.rm = TRUE),
    explosive_play_rate = mean(is_explosive, na.rm = TRUE),
    avg_epa = mean(epa, na.rm = TRUE),
    avg_defenders_in_box = mean(defenders_in_box, na.rm = TRUE),
    avg_pass_rate = mean(is_pass)
  ) |>
  ungroup()


make_context_vars = add_play_callers |>
  mutate(
    offense_score = ifelse(
      possession_team == home_team,
      total_home_score,
      total_away_score
    ),
    defense_score = ifelse(
      defteam == home_team,
      total_home_score,
      total_away_score
    ),
    is_home_team = ifelse(possession_team == home_team, 1, 0),
    diff = offense_score - defense_score,
    avg_diff = mean(diff),
    .by = c(off_play_caller, nflverse_game_id)
  ) |>
  group_by(off_play_caller) |>
  distinct(nflverse_game_id, .keep_all = TRUE) |>
  select(
    nflverse_game_id,
    game_date,
    stadium,
    roof,
    surface,
    div_game,
    home_team,
    wind,
    temp,
    is_home_team,
    avg_diff,
    off_play_caller,
    play_caller_tenure,
  ) |>
  ungroup() |>
  mutate(
    temp = case_when(
      roof %in% c('closed', 'dome') ~ 60,
      is.na(temp) ~ 68,
      .default = temp
    ),
    wind = case_when(
      roof %in% c('closed', 'dome') ~ 0,
      is.na(wind) ~ 8,
      .default = wind
    ),
    surface = str_squish(surface)
  )


cleaned_data = make_snap_shares |>
  left_join(make_features) |>
  left_join(make_context_vars) |>
  mutate(
    surface = case_when(
      nchar(surface) < 1 & stadium == "Levi'sÂ® Stadium" ~ 'grass',
      nchar(surface) < 1 & stadium == 'Tottenham Hotspur Stadium' ~ 'grass',
      nchar(surface) < 1 & stadium == 'Estadio Azteca (Mexico City)' ~ 'grass',
      nchar(surface) < 1 & stadium == 'MetLife Stadium' ~ 'fiedldturf',
      nchar(surface) < 1 &
        stadium == 'GEHA Field at Arrowhead Stadium' ~ 'grass',
      nchar(surface) < 1 & stadium == 'Soldier Field' ~ 'grass',
      nchar(surface) < 1 & stadium == "M&T Bank Stadium" ~ 'grass',
      nchar(surface) < 1 & stadium == "Empower Field at Mile High" ~ 'grass',
      nchar(surface) < 1 & stadium == "SoFi Stadium" ~ 'matrixturf',
      # spirtually this is still heinz field
      nchar(surface) < 1 & stadium == 'Acrisure Stadium' ~ 'grass',
      nchar(surface) < 1 & stadium == 'Paycor Stadium' ~ 'a_turf',
      # technically the superdome is now caesars but lets just be sure
      nchar(surface) < 1 &
        stadium == 'Mercedes-Benz Stadium' &
        home_team == 'ATL' ~ 'fieldturf',
      nchar(surface) < 1 & stadium == 'EverBank Stadium' ~ 'grass',
      nchar(surface) < 1 & stadium == 'Highmark Stadium' ~ 'astroturf',
      nchar(surface) < 1 & stadium == 'Gillette Stadium' ~ 'fieldturf',
      nchar(surface) < 1 & stadium == 'AT&T Stadium' ~ 'fieldturf',
      nchar(surface) < 1 & stadium == 'Ford Field' ~ 'fieldturf',
      nchar(surface) < 1 & stadium == 'Arena Corinthians' ~ 'grass',
      nchar(surface) < 1 & stadium == 'Allianz Arena' ~ 'grass',
      nchar(surface) < 1 & stadium == 'Lumen Field' ~ 'fieldturf',
      .default = surface
    ),
    surface = str_squish(surface),
    across(c(surface, roof), \(x) as.factor(x))
  ) |>
  select(-home_team)


```


There are a few things in here worth mentioning briefly. Instead of separating out explosive play rate into explosive pass and run I just keep them together and adjust for passing rate. Second I decided to consolidate all the possible surfaces and roofs into just outdoor and indoors. We would really only care if the game is outside, on-grass, and raining/snowing which would probably affect the rate of explosive plays. 

```{python}
#| label: clean-data-python
#| code-fold: true

seed = 14993111

RANDOM_SEED = np.random.default_rng(seed = seed)

def plot_prior(trace ,param: str):
    fig, axe = plt.subplots()
    return az.plot_dist(trace.prior[param])



raw_data = (
    pl.read_parquet('processed-data/processed-dat.parquet')
    .with_columns(
        pl.col('nflverse_game_id')
        .str.extract(r"_(\d{2})_")
        .str.replace_all('_', '')
        .str.to_integer()
        .alias('week'),
        pl.when(pl.col('surface') == 'grass')
        .then(pl.lit(1))
        .otherwise(0)
        .alias('is_grass'),
        pl.when(
        pl.col('roof').is_in(['closed', 'dome']))
        .then(pl.lit(1))
        .otherwise(0)
        .alias('is_indoors')    
            )

)   


raw_data_pd = raw_data.to_pandas()


unique_seasons = raw_data_pd['play_caller_tenure'].sort_values().unique()

unique_play_callers = raw_data_pd['off_play_caller'].sort_values().unique()

season_idx = pd.Categorical(
    raw_data_pd['play_caller_tenure'], categories=unique_seasons
).codes


unique_games = raw_data_pd['week'].sort_values().unique()

games_idx = pd.Categorical(
    raw_data_pd['week'], categories=unique_games).codes



coach_idx = pd.Categorical(
    raw_data_pd['off_play_caller'], categories=unique_play_callers).codes


predictors = ['avg_epa', 'avg_defenders_in_box',
            'is_indoors', 'is_grass', 'div_game',
            'wind', 'temp', 'is_home_team', 'avg_diff', 'avg_pass_rate']



just_controls = raw_data_pd[predictors]

nobs = raw_data.height

scaled_y = (
    raw_data
    .with_columns(
    (pl.col('explosive_play_rate') / (pl.col('explosive_play_rate').abs().max())).alias("scaled_explosive_plays")
    ).with_columns(
        (((pl.col('scaled_explosive_plays')) * (nobs-1) + 0.5)/nobs).alias("explosive_play_rate_transformed")
    )

)



get_stds = (
    raw_data
    .group_by('season')
    .agg(
        pl.col('explosive_play_rate').std()
    )

)


numeric_cols = (
    pl.from_pandas(just_controls)
    .select(
            pl.exclude('div_game', 'is_home_team', 'is_indoors', 'is_grass')).columns
)


means = (
    pl.from_pandas(just_controls)
    .select(
        [pl.col(c).mean().alias(c) for c in numeric_cols]
    )

)


stds = (
    pl.from_pandas(just_controls)
    .select(
        [pl.col(c).std().alias(c) for c in numeric_cols]
    )

)



just_controls_pl = pl.from_pandas(just_controls)



just_controls_sdz = (
    just_controls_pl
    .with_columns(
        [((pl.col(c) - means[0,c])/ stds[0, c]).alias(c) for c in numeric_cols]
    )
    .with_columns(
        pl.Series("div_game", just_controls_pl['div_game']),
        pl.Series('is_home_team', just_controls_pl['is_home_team']), 
        pl.Series('is_indoors', just_controls_pl['is_indoors']), 
        pl.Series ('is_grass', just_controls_pl['is_grass'])
    ).to_pandas()

)

personnel_cols = (
    raw_data.select(cs.starts_with('personnel')).to_pandas()
)
just_controls_sdz.columns

usage = personnel_cols.mean(axis = 0)

reliable_cols = usage[usage >= 0.01].index.tolist()

personnel_cols = personnel_cols[reliable_cols]


used_cols = (
    raw_data
    .select(cs.starts_with('personnel'))

)


usage = used_cols.mean()

mask_df = usage.select(
    [(pl.col(c) >= 0.01).alias(c) for c in usage.columns]

)

mask = mask_df.row(0, named = True)

reliable_cols = [name for name, keep in mask.items() if keep]

personnel_cols = used_cols.select(reliable_cols).columns

personnel_scaled = (
    raw_data
    .select(pl.col(personnel_cols))
    .with_columns(
        [
            (pl.col(c) / (pl.col(c).abs().max()))
            for c in personnel_cols
        ]
    ).to_pandas()
    

)



```

To speed this process along a little bit I then go and make a few binary indicators, standardize my controls, then apply the max absolute value scaling to my outcome. Since explosive play rate is a proportion we can just model that with the Beta distribution. The issue is that a Beta regression can't model degenerate outcomes. This is statistician for exactly equal to zero or exactly equal to one. 

```{python}

fig, axes = plt.subplots()
sns.histplot(data = scaled_y, x = 'explosive_play_rate_transformed')

```


To handle this I nudged the values of explosive play rate away from zero and one. There are definetly drawbacks to this approach as outlined in [Kubinec 2023](https://www.cambridge.org/core/journals/political-analysis/article/abs/ordered-beta-regression-a-parsimonious-wellfitting-model-for-continuous-data-with-lower-and-upper-bounds/89F4141DA16D4FC217809B5EB45EEE83), but we are still working on a native PyMC implementation. 

## The Model 

We are going to build the model bit by bit. Just so `PyMC` does not get mad at me we are going to define the data first. 

```{python}
#| code-fold: true
with pm.Model(coords = coords) as mmm_hsgp: 

    global_controls = pm.Data(
        'control_data', just_controls_sdz.drop('avg_pass_rate', axis = 1),
                        dims = ('obs_id', 'predictors')
    )

    passing_dat = pm.Data('passing_data',
                            just_controls_sdz['avg_pass_rate'],
                            dims = 'obs_id')

    personnel_dat = pm.Data(
        'personel_data', personnel_scaled, dims = ('obs_id','channels')
    )

    season_data = pm.Data('season_id', season_idx, dims = 'obs_id')

    game_data = pm.Data('game_id', games_idx, dims = 'obs_id' )

    x_seasons = pm.Data('x_seasons', unique_seasons, dims = 'seasons')[:,None]

    x_games = pm.Data('x_games', unique_games, dims = 'games')[:, None]

    obs_exp_plays = pm.Data('obs_exp_plays',
                            scaled_y['explosive_play_rate_transformed'].to_numpy(),
                            dims = 'obs_id')
```

The next step we are going to define the HSGPs for the games and tenure dimensions and the coach effects. 


```{python}
#| code-fold: True


def logit(p):
    return np.log(p / (1 - p))
with mmm_hsgp:
    gps_sigma = pm.Exponential('gps_sigma', 10, dims='time_scale')

    ls = pm.InverseGamma('ls',
                        alpha = np.array([in_season_gp.alpha,
                                        between_season_gp.alpha]),
                        beta = np.array([in_season_gp.beta,
                                        between_season_gp.beta]),
                        dims='time_scale')

    cov_coach_evo = gps_sigma[0]**2*pm.gp.cov.Matern52(input_dim=1, ls = ls[0])

    cov_game_evo = gps_sigma[1]**2*pm.gp.cov.Matern52(input_dim=1, ls = ls[1])

    gp_coach_evo = pm.gp.HSGP(m = [between_season_m],
                            c = between_season_c,
                            cov_func=cov_coach_evo)

    gp_game_evo = pm.gp.HSGP(m = [in_season_m],
                            c = in_season_c,
                            cov_func=cov_game_evo)

    coach_evolution = gp_coach_evo.prior('coach_evolution',
                                            X = x_seasons,
                                            dims = 'seasons')

    game_evolution = gp_game_evo.prior('game_evolution',
                                        X = x_games,
                                        dims = 'games')
    coach_mean_raw = pm.Normal('coach_mean_raw',
                                mu = logit(obs_exp_plays.mean()),
                                sigma = 0.5,
                                dims = 'play_callers')

    coach_mu = pm.Deterministic('coach_mu', 
                                coach_mean_raw[coach_idx] + 
                                coach_evolution[season_idx] +
                                game_evolution[games_idx],
                                dims = 'obs_id')


```

The next step is to manually build the adstocks for the personnel data. Here we are going to feed each of the personnel groupings through the adstock function. We are basically going to let the defenses look back 3ish games. In some ways this is likely a little restrictive. Teams may look back further than that to guess tendencies or how they play particular styles of defenses. You may see your divisional opponents in the first part of the season and then the final part of the season. 

However, I think that setting the maximum carryover effect to 3 strikes a nice balance. This roughly divides the 16-17 game schedule into 5ish overlapping chunks. Players tend to get hurt throughout the season forcing playcallers to adapt their personnel groupings. Shifts in the offensive philosophy may not have worked out that well or have been adapted to as people see these concepts more and more. The 3 games prior to the 17th maybe more informative than the games in the first three weeks of the season. How this works is that we are going to weight the game last week more than the game two weeks before, and so on and so forth depending on the size of the carryover effect we specify. 

Next we are going to transform it using a logistic saturation function. By doing this all we are saying is that there are only so many plays we can invest into a single personnel grouping during a game before we see diminishing returns. If you play 21 personnel for 70% of the game then moving to 80% is not going to result in a a 10% jump in explosive plays. 

```{python}


with mmm_hsgp:
    for i in range(len(coords['channels'])):
        cols = personnel_dat[:,i]
        adstock_list.append(geometric_adstock(cols, adstock_alphas[i],
                            l_max = 3))

    x_adstock = pt.stack(adstock_list, axis = 1)

    channel_betas = pm.Normal('channel_betas', mu = 0, sigma = 0.5,
                                dims = 'channels')

    saturation_lam = pm.Gamma('saturation_lamda',
                                                alpha = 3,
                                                beta = 0.5, dims = 'channels')

    saturation_slope = pm.LogNormal('saturation_slope',
                                        mu = 0,
                                        sigma = 0.2, 
                                        dims = 'channels')
    # effectivelly logistic saturation
    x_saturated = ((1-pm.math.exp(-saturation_lam * x_adstock)) / (1 + pm.math.exp(-saturation_lam * x_adstock)))



```

Then we are just going to fuse everthing together by taking our saturated x's and our prior on the channel contributions. In this case I separated out the pass rate from the rest of the controls just because I want it to vary a bit more. I ended up doing this just because it seemed to improve sampling and the overall fit of the posterior predictive. 


```{python}

with mmm_hsgp:
     personnel_contribution = pm.Deterministic(
        'personnel_contribution', 
        pm.math.dot(x_saturated, channel_betas),
        dims = 'obs_id'

    )

    passing_prior = pm.Normal('passing_prior', mu = 0, sigma = 0.5)

    controls_beta = pm.Normal('controls_beta', mu=0, sigma= 0.05,
    dims='predictors')

    control_contribution = pm.Deterministic(
        'control_contribution', 
        pm.math.dot(global_controls, controls_beta), 
        dims='obs_id'

    )



    mu = pm.Deterministic('mu',
        coach_mu +  
        personnel_contribution +  
        control_contribution + 
        (pm.math.dot(passing_prior, passing_dat)),      
        dims='obs_id'
    )
    # like 1/20 plays is explosive
    precision = pm.Exponential('precision', 1/20)
   
    mu_logit = pm.math.invlogit(mu)
  

    pm.Beta(
        'y_obs', 
        mu = mu_logit, 
        nu = precision,
        observed = obs_exp_plays,
        dims = 'obs_id')



```


Now we are just going to run the sampling. 


```{python}

with mmm_hsgp:
    idata = pm.sample_prior_predictive()
    idata.extend(
        pm.sample(random_seed=RANDOM_SEED, nuts_sampler='numpyro')
    )
    idata.extend(
        pm.sample_posterior_predictive(idata, compile_kwargs={"mode":"NUMBA"})
    )

```